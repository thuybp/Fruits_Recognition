{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_base = VGG16(weights='imagenet', include_top=False, input_shape=(100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_base.summary()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features (dir_path, samples):\n",
    "    feature_maps = np.zeros(shape=(samples,3,3,512))\n",
    "    labels = np.zeros(shape=(samples))\n",
    "    generator = datagen.flow_from_directory(dir_path, target_size=(100,100), batch_size=batch_size)\n",
    "    i = 0\n",
    "    for images_batch, labels_batch in generator:\n",
    "        features_batch = CNN_base.predict(inputs_batch)\n",
    "        features_maps[i*batch_size : (i+1)*batch_size] = features_batch\n",
    "        i += 1\n",
    "        if i*batch_size >= samples:\n",
    "            pass\n",
    "    return feature_maps, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a04c0a8c0627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48905\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8205\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8216\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extract_features(train_dir, 48905)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 8205)\n",
    "test_features, test_labels = extract_features(test_dir, 8216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8205//33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48905 images belonging to 95 classes.\n",
      "(128, 95)\n",
      "(128, 100, 100, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (128,95) into shape (128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d9aa7c2dfdf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# extract the outputs of VGG net for the inputs: training set, validation set and test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48905\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8205\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8216\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d9aa7c2dfdf0>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(dir_path, samples)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (128,95) into shape (128)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import MaxPooling2D, Conv2D\n",
    "from keras.layers import Activation, Dropout, regularizers, Flatten, Dense\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt \n",
    "from timeit import default_timer as timer\n",
    "import numpy as np \n",
    "\n",
    "CNN_base = VGG16(weights='imagenet',include_top=False, input_shape=(100,100,3))\n",
    "\n",
    "start_t = timer()\n",
    "\n",
    "\n",
    "# declare the paths of downloaded folder; Training, Validation and Test folder\n",
    "training_dir = '/Users/bathuy/Downloads/fruits-360/Training'\n",
    "validation_dir = '/Users/bathuy/Downloads/fruits-360/Validation'\n",
    "test_dir = '/Users/bathuy/Downloads/fruits-360/Test'\n",
    "\n",
    "# define the parameters\n",
    "saved_path = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'CNN_pretrained_VGG.h5'\n",
    "model_weights = 'CNN_scratch_pretrained_VGG_weights.h5'\n",
    "num_classes = 95\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 128\n",
    "\n",
    "# define a function to extract features of the CNN_base outputs\n",
    "def extract_features(dir_path, samples):\n",
    "    feature_maps = np.zeros(shape=(samples, 3, 3, 512))\n",
    "    labels = np.zeros(shape=(samples))\n",
    "    generator = datagen.flow_from_directory(dir_path, \n",
    "                            target_size=(100, 100), batch_size=batch_size)\n",
    "    i = 0\n",
    "    for images_batch, labels_batch in generator:\n",
    "        features_batch = CNN_base.predict(images_batch)\n",
    "        feature_maps[i*batch_size: (i+1)*batch_size] = features_batch\n",
    "        print(labels_batch.shape)\n",
    "        print(images_batch.shape)\n",
    "        labels[i*batch_size : (i+1)*batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i*batch_size >= samples:\n",
    "            break\n",
    "    return feature_maps, labels\n",
    "\n",
    "# extract the outputs of VGG net for the inputs: training set, validation set and test set\n",
    "train_features, train_labels = extract_features(training_dir, 48905)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 8205)\n",
    "test_features, test_labels = extract_features(test_dir, 8216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0ca9c81cb2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48905 images belonging to 95 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(training_dir, batch_size=batch_size, target_size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x134a31ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, label in train_generator:\n",
    "    print(images.shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ensc813-s2019]",
   "language": "python",
   "name": "conda-env-ensc813-s2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
